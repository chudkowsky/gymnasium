{
  "comment": "PPO training configuration for chess RL",
  "training": {
    "total_steps": 100000,
    "steps_per_update": 2048,
    "log_interval": 100,
    "checkpoint_interval": 1000,
    "device": "cpu",
    "seed": 42
  },
  "ppo": {
    "learning_rate": 3e-4,
    "n_epochs": 3,
    "batch_size": 64,
    "clip_ratio": 0.2,
    "entropy_coef": 0.01,
    "value_coef": 0.5,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "max_grad_norm": 0.5
  },
  "rollout": {
    "num_episodes_per_update": 32,
    "num_envs": 1,
    "max_episode_length": 300
  },
  "opponent_pool": {
    "types": ["random"],
    "sampling": "uniform",
    "comment": "Add 'snapshot' type to include checkpoint-based opponents"
  },
  "checkpoints": {
    "pretrained_path": "data/checkpoints/pretrained.pth",
    "trained_output_dir": "data/checkpoints",
    "save_format": "trained_ppo_step_{step}.pth"
  }
}
